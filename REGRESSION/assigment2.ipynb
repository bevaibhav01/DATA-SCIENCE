{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7ba47da-25a9-4d03-b529-898a7e597bce",
   "metadata": {},
   "source": [
    "Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it\n",
    "represent?\n",
    "\n",
    "ANS:-\n",
    "R-squared (R²) is a statistical measure used to assess the goodness of fit of a linear regression model. It indicates the proportion of the variance in the dependent variable (y) that is explained by the independent variables (x) included in the model. In other words, R-squared measures how well the independent variables explain the variability in the dependent variable.\n",
    "\n",
    "Calculation:\n",
    "R-squared is calculated as the ratio of the explained sum of squares (ESS) to the total sum of squares (TSS).\n",
    "ESS is the sum of squared differences between the predicted values of the dependent variable (obtained from the regression model) and the mean of the dependent variable.\n",
    "TSS is the sum of squared differences between the actual values of the dependent variable and the mean of the dependent variable.\n",
    "\n",
    "Interpretation:\n",
    "R-squared values range from 0 to 1. A value of 0 indicates that the independent variables in the model do not explain any of the variability in the dependent variable, while a value of 1 indicates that the independent variables explain all of the variability.\n",
    "Higher R-squared values indicate a better fit of the model to the data, as they indicate that a larger proportion of the variance in the dependent variable is explained by the independent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b44d33-d8d4-4cf9-b4c1-70c4e88c34e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2c408baf-57bf-40f5-a3d8-7f3182fef0aa",
   "metadata": {},
   "source": [
    "Q2. Define adjusted R-squared and explain how it differs from the regular R-squared.\n",
    "\n",
    "Adjusted R-squared is a modified version of the regular R-squared (R²) that adjusts for the number of predictor variables in the model. While R-squared measures the proportion of variance in the dependent variable explained by the independent variables, adjusted R-squared takes into account the complexity of the model by penalizing the inclusion of additional predictors that do not significantly improve the model's fit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d637fb1c-a99b-43bd-89d7-3559e2da670b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "767c132a-50f6-4fd3-be68-65f0ce283d5f",
   "metadata": {},
   "source": [
    "Q3. When is it more appropriate to use adjusted R-squared?\n",
    "\n",
    "Abjusted is more appropriate when the number of features are more we need the accuracy to be accurate so when features are more adjusted r2 will no increase much when feature is not that much correlated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a84490-060f-4169-82a9-93890bdaf23f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b074470f-e0ee-4bb1-bc4a-2742d655b53c",
   "metadata": {},
   "source": [
    "Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics\n",
    "calculated, and what do they represent?\n",
    "\n",
    "ans:-\n",
    "RMSE (Root Mean Squared Error), MSE (Mean Squared Error), and MAE (Mean Absolute Error) are commonly used metrics for evaluating the performance of regression models. They measure the differences between the actual values of the dependent variable and the predicted values produced by the model.\n",
    "\n",
    "MAE is robust to ouliers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e302fb7-ad41-4007-bb07-29434183d13a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "afc99fc5-0ca1-42ad-8295-8fe8219620fa",
   "metadata": {},
   "source": [
    "Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in\n",
    "regression analysis.\n",
    "\n",
    "ans:-\n",
    "RMSE and MSE are advantageous when large errors are particularly undesirable, as they penalize larger errors more heavily. However, they are sensitive to outliers due to the squaring operation.\n",
    "\n",
    "MAE is advantageous when outliers are present or when minimizing large errors is not a priority. It provides a more robust measure of the average magnitude of errors.\n",
    "\n",
    "The choice of metric depends on the specific characteristics of the data, the modeling objectives, and the relative importance of different types of errors. It is often recommended to consider multiple metrics to get a comprehensive understanding of the model's performance.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b33a6f6-b0cb-4172-b448-39c3856a405a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d4a3e374-4794-4516-af77-ae2d361802cb",
   "metadata": {},
   "source": [
    "Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is\n",
    "it more appropriate to use?\n",
    "\n",
    "\n",
    "Lasso (Least Absolute Shrinkage and Selection Operator) regularization is a technique used in linear regression and other regression models to prevent overfitting by imposing a penalty on the absolute values of the coefficients. It adds a regularization term to the loss function, which penalizes large coefficient values, encouraging simpler and more interpretable models.\n",
    "\n",
    "Ridge regularization adds a penalty term proportional to the squared values of the coefficients (L2 norm) to the loss function. This penalty term encourages smaller coefficient values but does not lead to exact zero coefficients.\n",
    "Unlike Ridge regularization, which shrinks all coefficients towards zero, Lasso regularization can lead to sparse solutions with some coefficients set exactly to zero. This property makes Lasso useful for feature selection.\n",
    "Ridge regularization is more effective at reducing multicollinearity among predictor variables, while Lasso tends to arbitrarily choose one of the correlated variables and shrink the coefficients of others to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae7254a-a623-4c0f-bc16-bd02a5e5cbee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5fdceca7-8aa6-49f0-9d7e-6643deb859d7",
   "metadata": {},
   "source": [
    "Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an\n",
    "example to illustrate.\n",
    "\n",
    "Regularized linear models help prevent overfitting in machine learning by adding a penalty term to the loss function, which discourages complex models with large coefficients. This penalty encourages simpler models that generalize well to unseen data, reducing the risk of overfitting.\n",
    "\n",
    "Overfitting occurs when a model learns to capture noise and random fluctuations in the training data, leading to poor performance on unseen data.\n",
    "Regularization techniques, such as Ridge (L2 regularization) and Lasso (L1 regularization), add penalty terms to the loss function, which penalize large coefficients.\n",
    "By penalizing large coefficients, regularization discourages overly complex models that fit the noise in the training data too closely.\n",
    "This encourages the model to generalize better to unseen data, leading to improved performance and reduced overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea56ff96-b0b9-4310-9e3a-9e21e55b0ded",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b6f7c623-42a8-43ce-a748-44f7c0025373",
   "metadata": {},
   "source": [
    "Q8. Discuss the limitations of regularized linear models and explain why they may not always be the best\n",
    "choice for regression analysis.\n",
    "\n",
    "Difficulty Handling High-Dimensional Data:\n",
    "While regularized linear models are effective at handling high-dimensional data, they may struggle with datasets with a very high number of features relative to the number of observations. In such cases, feature selection becomes more challenging, and the model may be prone to overfitting or underfitting.\n",
    "\n",
    "Lack of Interpretability:\n",
    "Regularized linear models, especially Lasso regression, can lead to sparse solutions with some coefficients set to zero. While this can be beneficial for feature selection, it may also reduce the interpretability of the model, as some predictors are excluded from the model entirely.\n",
    "\n",
    "Linearity Assumption:\n",
    "Regularized linear models assume a linear relationship between the predictors and the target variable. If the true relationship is highly nonlinear, using a linear model may result in poor performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d2c587-61a8-4624-8d57-ba85e96533ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bd7e3cac-4d41-447b-b101-aec60e187dee",
   "metadata": {},
   "source": [
    "Q9. You are comparing the performance of two regression models using different evaluation metrics.\n",
    "Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better\n",
    "performer, and why? Are there any limitations to your choice of metric?\n",
    "\n",
    "\n",
    "In this scenario, Model B (MAE of 8) would likely be chosen as the better performer because it has a lower average error compared to Model A (RMSE of 10). A smaller error indicates better predictive accuracy, and MAE is a more straightforward metric that provides a direct measure of the average magnitude of errors without the squaring operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ff1978-08b3-4cb7-9f3a-d4608cf5f9e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4a5448de-12e4-4953-844d-494becb21915",
   "metadata": {},
   "source": [
    "Q10. You are comparing the performance of two regularized linear models using different types of\n",
    "regularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B\n",
    "uses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the\n",
    "better performer, and why? Are there any trade-offs or limitations to your choice of regularization\n",
    "method?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ddfe64-9124-45fb-90bd-c7f19ab701dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68357ff6-c1c3-421b-82ec-51aa65235226",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5f2920-c688-4a37-aa9e-2f04ccde2011",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
