{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7446d004-ec42-4d8a-bffa-dc8972789ffe",
   "metadata": {},
   "source": [
    "Q1. What is Lasso Regression, and how does it differ from other regression techniques?\n",
    "\n",
    "Lasso regression, short for Least Absolute Shrinkage and Selection Operator, is a linear regression technique that incorporates regularization to improve model performance and feature selection. It differs from other regression techniques, particularly ordinary least squares (OLS) regression, in the following ways:\n",
    "\n",
    "Lasso regression adds a penalty term proportional to the absolute values of the coefficients (L1 norm) to the ordinary least squares loss function.\n",
    "This penalty term encourages sparsity in the coefficient estimates by shrinking some coefficients exactly to zero, effectively performing feature selection.\n",
    "In contrast, OLS regression does not include any regularization penalty, and all coefficients are estimated solely based on minimizing the sum of squared differences between the actual and predicted values.\n",
    "One of the key advantages of Lasso regression is its ability to perform feature selection by setting some coefficients to exactly zero.\n",
    "By effectively eliminating less important features from the model, Lasso regression can produce simpler and more interpretable models, especially in high-dimensional datasets where there are many predictors relative to the number of observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111e9f1f-a549-468d-9804-3cc7873538ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b8d96df1-ad7f-4d6a-8cd3-781a9f02f92d",
   "metadata": {},
   "source": [
    "Q2. What is the main advantage of using Lasso Regression in feature selection?\n",
    "\n",
    "Lasso Regression introduces a penalty term proportional to the absolute values of the coefficients (L1 norm) to the ordinary least squares loss function.\n",
    "This penalty term encourages sparsity in the coefficient estimates by shrinking some coefficients exactly to zero.\n",
    "Features with coefficients set to zero are effectively excluded from the model, as their contribution to predicting the target variable is considered negligible.\n",
    "Therefore, Lasso Regression performs automatic feature selection by identifying and retaining only the most important predictors while discarding less influential ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706bea91-8b1e-4d17-9e4d-2c8e161dffb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6fd47811-56dc-4eeb-900b-ccf6e0d975fb",
   "metadata": {},
   "source": [
    "Q3. How do you interpret the coefficients of a Lasso Regression model?\n",
    "\n",
    "Interpreting the coefficients of a Lasso Regression model involves understanding their magnitude, sign, and relevance to the target variable. However, due to the regularization process of Lasso Regression, there are some unique considerations compared to ordinary least squares (OLS) regression. Here's how to interpret the coefficients of a Lasso Regression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51be0e3-87cc-4f10-8be2-f38a1f79a24f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "96ecc57f-1ace-4b81-b617-842630c79a0c",
   "metadata": {},
   "source": [
    "Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the\n",
    "model's performance?\n",
    "\n",
    "In Lasso Regression, there are primarily two tuning parameters that can be adjusted to control the behavior of the model and its performance:\n",
    "Regularization Parameter (Lambda or Alpha):\n",
    "The regularization parameter, often denoted as lambda (λ) or alpha (α), controls the strength of the regularization penalty in Lasso Regression.\n",
    "A larger value of the regularization parameter increases the penalty on the absolute values of the coefficients (L1 norm), leading to more coefficients being shrunk towards zero.\n",
    "Increasing the regularization parameter results in more aggressive feature selection, as more coefficients are set exactly to zero, effectively excluding irrelevant predictors from the model.\n",
    "\n",
    "\n",
    "Max Iterations (Max_iter):\n",
    "The maximum number of iterations, often denoted as max_iter, specifies the maximum number of iterations or optimization steps taken by the optimization algorithm (such as coordinate descent or gradient descent) to converge to the optimal solution.\n",
    "Increasing the max_iter parameter may allow the optimization algorithm to converge to a more accurate solution, especially for complex or high-dimensional datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37894a0-6ccc-422b-b783-f3ce383b85b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "19abccd6-3c4d-4a6b-a061-dff6759cf0f0",
   "metadata": {},
   "source": [
    "Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?\n",
    "\n",
    "Lasso Regression, like other linear regression techniques, is inherently a linear method that models the relationship between predictor variables and the target variable using linear combinations of the predictors. However, it can be extended to handle non-linear regression problems through several techniques:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c6bfd6-e23f-4c48-97df-5dec8849049a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b450e545-0c35-4a04-ac83-d2645e2b604b",
   "metadata": {},
   "source": [
    "Q6. What is the difference between Ridge Regression and Lasso Regression?\n",
    "\n",
    "Regularization Penalty:\n",
    "Ridge Regression (L2 regularization): In Ridge Regression, a penalty term proportional to the squared values of the coefficients (L2 norm) is added to the ordinary least squares loss function.\n",
    "Lasso Regression (L1 regularization): In Lasso Regression, a penalty term proportional to the absolute values of the coefficients (L1 norm) is added to the ordinary least squares loss function.\n",
    "\n",
    "Ridge Regression: Ridge Regression shrinks the coefficients towards zero by applying a penalty that encourages smaller coefficient values. However, it does not enforce sparsity in the coefficient estimates, meaning that all coefficients are reduced but not necessarily set to zero.\n",
    "Lasso Regression: Lasso Regression not only shrinks the coefficients but also performs feature selection by setting some coefficients exactly to zero. This sparsity-inducing property of Lasso Regression results in a more parsimonious model with fewer predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d683234-1ac3-47f5-b821-0d3ff9ccedd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2250d344-69b7-4dbb-921f-1107d742d24a",
   "metadata": {},
   "source": [
    "Q7. Can Lasso Regression handle multicollinearity in the input features? If yes, how?\n",
    "\n",
    "Yes, Lasso Regression can handle multicollinearity in the input features to some extent, although its approach differs from that of Ridge Regression. Multicollinearity occurs when predictor variables are highly correlated with each other, which can lead to instability in coefficient estimates in linear regression models. Here's how Lasso Regression addresses multicollinearity:\n",
    "\n",
    "Feature Selection:\n",
    "One of the key properties of Lasso Regression is its ability to perform feature selection by setting some coefficients exactly to zero.\n",
    "In the presence of multicollinearity, Lasso Regression tends to select a subset of features while setting the coefficients of less important or redundant predictors to zero.\n",
    "By excluding less relevant predictors from the model, Lasso Regression can mitigate the effects of multicollinearity and produce more stable coefficient estimates.\n",
    "Shrinkage of Coefficients:\n",
    "Lasso Regression shrinks the coefficients of predictor variables towards zero by applying a penalty term proportional to the absolute values of the coefficients (L1 norm).\n",
    "In the presence of multicollinearity, Lasso Regression redistributes the influence of correlated predictors by shrinking their coefficients towards zero.\n",
    "This shrinkage helps stabilize the coefficient estimates and reduces their sensitivity to small changes in the input features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b551996-ad17-4459-ac58-c8b973142182",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7dd1fcc6-7891-4c6e-b0ce-d5f49327aae1",
   "metadata": {},
   "source": [
    "Q8. How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?\n",
    "\n",
    "HYPERPARAMETRE TUNING\n",
    "cross fold\n",
    "grid search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6692d2d7-d2f0-446c-898d-7baeb582bab2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f7a5a3-39fa-4519-89a3-8f4d47b3893f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2614f2aa-802d-4f53-838d-abe0ba49cff7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41d5b50-c58e-4a10-a74f-d3f0dc82c1d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
